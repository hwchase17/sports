{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may need to change\n",
    "sys.path.insert(0, \"/Users/harrisonchase/workplace/sports/\")\n",
    "\n",
    "from clean_sports_work.sports_reference.api import find_table, extract_table, create_insert_table_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [04:34<00:00,  5.01s/it]\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "for year in tqdm.tqdm(range(1950, 2020)):\n",
    "    url = 'https://www.basketball-reference.com/leagues/NBA_{}_advanced.html'.format(year)\n",
    "    html = urlopen(url)\n",
    "\n",
    "    # create the BeautifulSoup object\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    table_str = find_table(soup, 'advanced_stats')\n",
    "\n",
    "    yr2018 = extract_table(table_str, header_row=0, get_url=True, start_of_rows=1)\n",
    "    drop_cols = yr2018.isnull().mean()[lambda x: x == 1].index\n",
    "    for col in drop_cols:\n",
    "        del yr2018[col]\n",
    "    yr2018['year'] = year\n",
    "    all_dfs.append(yr2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [02:08<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "all_draft_dfs = []\n",
    "for year in tqdm.tqdm(range(1950, 2020)):\n",
    "    url = 'https://www.basketball-reference.com/draft/NBA_{}.html'.format(year)\n",
    "    html = urlopen(url)\n",
    "\n",
    "    # create the BeautifulSoup object\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    table_str = find_table(soup, 'stats')\n",
    "\n",
    "    yr2018 = extract_table(table_str, header_row=1, get_url=True, start_of_rows=2)\n",
    "    drop_cols = yr2018.isnull().mean()[lambda x: x == 1].index\n",
    "    for col in drop_cols:\n",
    "        del yr2018[col]\n",
    "    yr2018['year'] = year\n",
    "    all_draft_dfs.append(yr2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:37<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "mvp_dfs = []\n",
    "mip_dfs = []\n",
    "roy_dfs = []\n",
    "for year in tqdm.tqdm(range(1960, 2020)):\n",
    "    url = 'https://www.basketball-reference.com/awards/awards_{}.html'.format(year)\n",
    "    html = urlopen(url)\n",
    "\n",
    "    # create the BeautifulSoup object\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    try:\n",
    "        table_str = find_table(soup, 'mvp')\n",
    "\n",
    "        yr2018 = extract_table(table_str, header_row=1, get_url=True, start_of_rows=2)\n",
    "        drop_cols = yr2018.isnull().mean()[lambda x: x == 1].index\n",
    "        for col in drop_cols:\n",
    "            del yr2018[col]\n",
    "        yr2018['year'] = year\n",
    "        mvp_dfs.append(yr2018)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        table_str = find_table(soup, 'mip')\n",
    "\n",
    "        yr2018 = extract_table(table_str, header_row=1, get_url=True, start_of_rows=2)\n",
    "        drop_cols = yr2018.isnull().mean()[lambda x: x == 1].index\n",
    "        for col in drop_cols:\n",
    "            del yr2018[col]\n",
    "        yr2018['year'] = year\n",
    "        mip_dfs.append(yr2018)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        table_str = find_table(soup, 'roy')\n",
    "\n",
    "        yr2018 = extract_table(table_str, header_row=1, get_url=True, start_of_rows=2)\n",
    "        drop_cols = yr2018.isnull().mean()[lambda x: x == 1].index\n",
    "        for col in drop_cols:\n",
    "            del yr2018[col]\n",
    "        yr2018['year'] = year\n",
    "        roy_dfs.append(yr2018)\n",
    "    except: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [02:46<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "playoff_dfs = []\n",
    "for year in tqdm.tqdm(range(1950, 2020)):\n",
    "    url = 'https://www.basketball-reference.com/playoffs/NBA_{}_advanced.html'.format(year)\n",
    "    html = urlopen(url)\n",
    "\n",
    "    # create the BeautifulSoup object\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    table_str = find_table(soup, 'advanced_stats')\n",
    "\n",
    "    yr2018 = extract_table(table_str, header_row=0, get_url=True, start_of_rows=1)\n",
    "    drop_cols = yr2018.isnull().mean()[lambda x: x == 1].index\n",
    "    for col in drop_cols:\n",
    "        del yr2018[col]\n",
    "    yr2018['year'] = year\n",
    "    playoff_dfs.append(yr2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough cleaning of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = pd.concat(all_dfs)\n",
    "all_stats = all_stats.replace('', np.nan)\n",
    "all_stats = all_stats.dropna(subset=['player_url', 'mp', 'age'])\n",
    "float_cols = ['bpm', 'ts_pct', 'per', 'usg_pct', 'obpm', 'dbpm', \n",
    "              'fg3a_per_fga_pct', 'fta_per_fga_pct', 'orb_pct', 'drb_pct',\n",
    "             'trb_pct', 'ast_pct', 'stl_pct', 'blk_pct', 'tov_pct', 'ws', 'ows', 'dws']\n",
    "for col in float_cols:\n",
    "    all_stats[col] = all_stats[col].astype(float)\n",
    "    \n",
    "int_cols = ['mp', 'age']\n",
    "for col in int_cols:\n",
    "    all_stats[col] = all_stats[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "playoff_stats = pd.concat(playoff_dfs)\n",
    "playoff_stats = playoff_stats.replace('', np.nan)\n",
    "playoff_stats = playoff_stats.dropna(subset=['player_url', 'mp', 'age'])\n",
    "float_cols = ['bpm', 'ts_pct', 'per', 'usg_pct', 'obpm', 'dbpm', \n",
    "              'fg3a_per_fga_pct', 'fta_per_fga_pct', 'orb_pct', 'drb_pct',\n",
    "             'trb_pct', 'ast_pct', 'stl_pct', 'blk_pct', 'tov_pct', 'ws', 'ows', 'dws']\n",
    "for col in float_cols:\n",
    "    playoff_stats[col] = playoff_stats[col].astype(float)\n",
    "    \n",
    "int_cols = ['mp', 'age']\n",
    "for col in int_cols:\n",
    "    playoff_stats[col] = playoff_stats[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats['pos'] = all_stats['pos'].str.split('-').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvp_df = pd.concat(mvp_dfs)\n",
    "mip_df = pd.concat(mip_dfs)\n",
    "roy_df = pd.concat(roy_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvp_df['award_share'] = mvp_df['award_share'].astype(float)\n",
    "mip_df['award_share'] = mip_df['award_share'].astype(float)\n",
    "roy_df['award_share'] = roy_df['award_share'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_draft = pd.concat(all_draft_dfs).dropna(subset=['player_url'])\n",
    "all_draft = all_draft.replace('', np.nan)\n",
    "all_draft = all_draft.drop_duplicates(subset=['player_url'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_draft['pick_overall'] = all_draft['pick_overall'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = all_stats.merge(all_draft[['player_url', 'pick_overall', 'college_name']], how='left', on='player_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create id for each (player, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats['id'] = all_stats['player_url'] + '___' + all_stats['year'].astype(str)\n",
    "playoff_stats['id'] = playoff_stats['player_url'] + '___' + playoff_stats['year'].astype(str)\n",
    "mvp_df['id'] = mvp_df['player_url'] + '___' + mvp_df['year'].astype(str)\n",
    "mip_df['id'] = mip_df['player_url'] + '___' + mip_df['year'].astype(str)\n",
    "roy_df['id'] = roy_df['player_url'] + '___' + roy_df['year'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playoff stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = all_stats.merge(playoff_stats[['id', 'mp', 'bpm']].rename(columns={'mp':'playoff_mp', 'bpm': 'playoff_bpm'}), how='left', on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicate ids\n",
    "\n",
    "Occurs if player played on multiple teams in one season, take row where team == 'TOT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_ids = all_stats['id'].value_counts()[lambda x: x> 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonchase/workplace/.venvs/record_linking/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/harrisonchase/workplace/.venvs/record_linking/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "changed = all_stats[all_stats['id'].isin(multiple_ids) & (all_stats['team_id'] == 'TOT')]\n",
    "first_team = all_stats.iloc[changed.index + 1]\n",
    "if not all(first_team['player_url'].values == changed['player_url'].values):\n",
    "    raise ValueError\n",
    "changed['started_team'] = first_team['team_id'].values\n",
    "no_change = all_stats[~all_stats['id'].isin(multiple_ids)]\n",
    "no_change['started_team'] = no_change['team_id']\n",
    "base_all_stats = pd.concat([\n",
    "    no_change,\n",
    "    changed,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_ids = base_all_stats[base_all_stats['mp'].notnull()]['team_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "team_renamer = dict()\n",
    "for team_id in set(team_ids).difference({'TOT'}):\n",
    "    url = 'https://www.basketball-reference.com/teams/{}/'.format(team_id)\n",
    "    html = urlopen(url)\n",
    "\n",
    "    # create the BeautifulSoup object\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    s = soup.find('script').text\n",
    "    if (s is not None) and ('teams/' in s):\n",
    "        regexp = re.compile(\"teams/(.*)/\")\n",
    "        s1 = regexp.search(s).group(1)\n",
    "        team_renamer[team_id] = s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_all_stats['team_id'] = base_all_stats['team_id'].apply(team_renamer.get).fillna(base_all_stats['team_id'])\n",
    "base_all_stats['started_team'] = base_all_stats['started_team'].apply(team_renamer.get).fillna(base_all_stats['started_team'])\n",
    "\n",
    "base_all_stats['started_team_url'] = '/teams/' + base_all_stats['started_team'] + '/' + base_all_stats['year'].astype(str) +'.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add team data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_ids = base_all_stats[base_all_stats['mp'].notnull()]['team_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_dfs = []\n",
    "for team_id in set(team_ids).difference({'TOT'}):\n",
    "    url = 'https://www.basketball-reference.com/teams/{}/'.format(team_id)\n",
    "    html = urlopen(url)\n",
    "\n",
    "    # create the BeautifulSoup object\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    table_str = find_table(soup, team_id)\n",
    "\n",
    "    yr2018 = extract_table(table_str, header_row=0, get_url=True, start_of_rows=1)\n",
    "\n",
    "    drop_cols = set(yr2018.isnull().mean()[lambda x: x == 1].index)\n",
    "    for col in drop_cols:\n",
    "        del yr2018[col]\n",
    "\n",
    "    yr2018['year'] = yr2018['lg_id_url'].str[-9:-5].astype(int)\n",
    "    team_dfs.append(yr2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_team_df = pd.concat(team_dfs).rename(columns={'team_name_url': 'started_team_url'})\n",
    "team_urls = all_team_df[all_team_df['year'] == 2020]['started_team_url'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_team_info = []\n",
    "for team_url in team_urls:\n",
    "\n",
    "    url = 'https://www.basketball-reference.com' + team_url\n",
    "\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    table_str = find_table(soup, 'roster')\n",
    "    yr2018 = extract_table(table_str, header_row=0, get_url=True, start_of_rows=1)\n",
    "\n",
    "    yr2018['team_url'] = team_url\n",
    "    new_team_info.append(yr2018[['player_url', 'team_url', 'years_experience', 'pos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonchase/workplace/.venvs/record_linking/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/harrisonchase/workplace/.venvs/record_linking/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/harrisonchase/workplace/.venvs/record_linking/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "most_recent_year = base_all_stats[base_all_stats['year'] == base_all_stats['year'].max()]\n",
    "most_recent_year['year'] += 1\n",
    "most_recent_year['age'] +=1\n",
    "most_recent_year['id'] = most_recent_year['player_url'] + '___' + most_recent_year['year'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_team_df = pd.concat(new_team_info)\n",
    "new_team_df['year'] = 2020\n",
    "new_team_df['id'] = new_team_df['player_url'] + '___' + new_team_df['year'].astype(str)\n",
    "new_team_df['pos'] = new_team_df['pos'].str.split('-').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_recent_year =new_team_df.drop(['pos'], 1).merge(most_recent_year.drop(['player_url', 'year'], 1), how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_year = most_recent_year[most_recent_year['years_experience'] != 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_year['team_id'] = most_recent_year['team_url'].str[7:10]\n",
    "most_recent_year['started_team'] = most_recent_year['team_id']\n",
    "most_recent_year['team_id_url'] = most_recent_year['team_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_year['team_id'] = most_recent_year['team_id'].apply(team_renamer.get).fillna(most_recent_year['team_id'])\n",
    "most_recent_year['started_team'] = most_recent_year['started_team'].apply(team_renamer.get).fillna(most_recent_year['started_team'])\n",
    "\n",
    "most_recent_year['started_team_url'] = '/teams/' + most_recent_year['started_team'] + '/' + most_recent_year['year'].astype(str) +'.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in most_recent_year.columns:\n",
    "    if col not in ['player', 'pos', 'age', 'team_id', 'id', 'player_url', 'pick_overall', 'college_name', 'started_team', 'year']:\n",
    "        most_recent_year[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in most_recent_year.iterrows():\n",
    "    if pd.isnull(row['age']):\n",
    "        sub = base_all_stats[base_all_stats['player_url'] == row['player_url']].sort_values('year')\n",
    "        if sub.empty:\n",
    "            raise ValueError\n",
    "        else:\n",
    "            sub_row = sub.iloc[-1]\n",
    "        for col in ['pos', 'player', 'pick_overall', 'college_name']:\n",
    "            most_recent_year.loc[i, col] = sub_row[col]\n",
    "        most_recent_year.loc[i, 'age'] = sub_row['age'] + 2020 - sub_row['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonchase/workplace/.venvs/record_linking/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_stats = pd.concat([base_all_stats, most_recent_year])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_team_df = pd.concat(team_dfs).rename(columns={'team_name_url': 'started_team_url'})\n",
    "all_team_df['team_url'] = all_team_df['started_team_url']\n",
    "all_team_df['team_id'] = all_team_df['started_team_url'].str[7:10]\n",
    "all_team_df['team_id'] = all_team_df['team_id'].apply(team_renamer.get).fillna(all_team_df['team_id'])\n",
    "all_team_df['started_team_url'] = '/teams/' + all_team_df['team_id'] + '/' + all_team_df['year'].astype(str) + '.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_team_df['made_playoffs'] = all_team_df['rank_team_playoffs'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_team_df = all_team_df.replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['win_loss_pct', 'srs', 'pace_rel', 'off_rtg_rel', 'def_rtg_rel', 'made_playoffs']:\n",
    "    all_team_df[col] = all_team_df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_team_df = all_team_df[['started_team_url', 'win_loss_pct', 'srs', 'pace_rel', 'off_rtg_rel', 'def_rtg_rel', 'made_playoffs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = all_stats.merge(filtered_team_df, how='left', on='started_team_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coach info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "coaches_by_year = all_team_df[['started_team_url', 'coaches_url', 'year']].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "coaches_by_year['team_id'] = coaches_by_year['started_team_url'].str[7:10]\n",
    "coaches_by_year['team_id'] = coaches_by_year['team_id'].apply(team_renamer.get).fillna(coaches_by_year['team_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_year = coaches_by_year.copy()\n",
    "p_year['year'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = coaches_by_year.merge(p_year, how='left', on=['team_id', 'year'], suffixes=('', '_p'))\n",
    "merged = merged.iloc[::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['year_coaching'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = np.nan\n",
    "p_t = np.nan\n",
    "yr_coaching = 0\n",
    "for i, row in merged.iterrows():\n",
    "    if pd.isnull(prev):\n",
    "        merged.loc[i, 'year_coaching'] = 0\n",
    "        yr_coaching = 0\n",
    "    elif (row['coaches_url'] == row['coaches_url_p']) and (p_t == row['team_id']):\n",
    "        yr_coaching +=1\n",
    "        merged.loc[i, 'year_coaching'] = yr_coaching\n",
    "    else:\n",
    "        merged.loc[i, 'year_coaching'] = 0\n",
    "        yr_coaching = 0\n",
    "    prev = row['coaches_url']\n",
    "    p_t = row['team_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = {\n",
    "    'CLE', 'LAL', 'MEM', 'MIN', 'PHO', 'SAC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in new:\n",
    "    mask = (merged['team_id'] == n) & (merged['year'] == 2020)\n",
    "    if sum(mask) != 1:\n",
    "        raise ValueError\n",
    "    merged.loc[mask, 'year_coaching'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_team = all_stats.merge(merged[['started_team_url', 'year_coaching']], how='left', on='started_team_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_team = all_stats_team.merge(\n",
    "    mvp_df[['id', 'award_share']].rename(columns={'award_share': 'mvp_award_share'}),\n",
    "    how='left', on='id'\n",
    ")\n",
    "all_stats_team = all_stats_team.merge(\n",
    "    mip_df[['id', 'award_share']].rename(columns={'award_share': 'mip_award_share'}),\n",
    "    how='left', on='id'\n",
    ")\n",
    "all_stats_team = all_stats_team.merge(\n",
    "    roy_df[['id', 'award_share']].rename(columns={'award_share': 'roy_award_share'}),\n",
    "    how='left', on='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_team.to_msgpack('all_stats.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
